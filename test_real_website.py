# -*- coding: utf-8 -*-
"""
ì‹¤ì œ KLOA ì›¹ì‚¬ì´íŠ¸ì—ì„œ HTML ê¸ì–´ì™€ì„œ íŒŒì‹± í…ŒìŠ¤íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

def fetch_kloa_merchant_page():
    """KLOA ìƒì¸ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
    url = "https://kloa.gg/merchant"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        print("ğŸŒ KLOA ìƒì¸ í˜ì´ì§€ì— ì ‘ì† ì¤‘...")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        print(f"âœ… ì‘ë‹µ ì½”ë“œ: {response.status_code}")
        print(f"ğŸ“„ HTML í¬ê¸°: {len(response.text):,} ë°”ì´íŠ¸")
        
        return response.text
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨: {e}")
        return None

def analyze_html_structure(html_content):
    """HTML êµ¬ì¡° ë¶„ì„"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    print("\nğŸ” HTML êµ¬ì¡° ë¶„ì„:")
    print("=" * 50)
    
    # í˜ì´ì§€ ì œëª©
    title = soup.find('title')
    if title:
        print(f"ğŸ“‹ í˜ì´ì§€ ì œëª©: {title.get_text()}")
    
    # ìƒì¸ ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
    print("\nğŸª ìƒì¸ ê´€ë ¨ ìš”ì†Œ ê²€ìƒ‰:")
    
    # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ìƒì¸ ì •ë³´ ì°¾ê¸°
    selectors_to_try = [
        'div[class*="merchant"]',
        'div[class*="trader"]',
        'div[class*="npc"]',
        'div[class*="px-8"]',
        'div[class*="border-b"]',
        '[data-merchant]',
        '[data-npc]',
        'div:contains("ì•„ë¥´í…Œë¯¸ìŠ¤")',
        'div:contains("ìœ ë””ì•„")',
        'div:contains("ë£¨í…Œë€")',
    ]
    
    for selector in selectors_to_try:
        try:
            elements = soup.select(selector)
            if elements:
                print(f"  âœ… '{selector}': {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
            else:
                print(f"  âŒ '{selector}': ìš”ì†Œ ì—†ìŒ")
        except Exception as e:
            print(f"  âš ï¸ '{selector}': ì˜¤ë¥˜ - {e}")
    
    # í…ìŠ¤íŠ¸ì—ì„œ ì§€ì—­ëª… ê²€ìƒ‰
    print("\nğŸ“ ì§€ì—­ëª… ê²€ìƒ‰:")
    regions = ['ì•„ë¥´í…Œë¯¸ìŠ¤', 'ìœ ë””ì•„', 'ë£¨í…Œë€', 'í† í† ì´í¬', 'ì• ë‹ˆì¸ ', 'í˜ì´íŠ¼', 'í‘¸ë¥¸ë°”ë‹¤']
    
    for region in regions:
        if region in html_content:
            print(f"  âœ… '{region}' ë°œê²¬")
        else:
            print(f"  âŒ '{region}' ì—†ìŒ")

def find_merchant_containers(html_content):
    """ìƒì¸ ì»¨í…Œì´ë„ˆ ìš”ì†Œë“¤ ì°¾ê¸°"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    print("\nğŸ¯ ìƒì¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°:")
    print("=" * 50)
    
    # ê°€ëŠ¥í•œ ìƒì¸ ì»¨í…Œì´ë„ˆ íŒ¨í„´ë“¤
    patterns = [
        # í´ë˜ìŠ¤ ê¸°ë°˜
        {'class': lambda x: x and 'px-8' in ' '.join(x)},
        {'class': lambda x: x and 'py-3' in ' '.join(x)},
        {'class': lambda x: x and 'border-b' in ' '.join(x)},
        {'class': lambda x: x and 'flex' in ' '.join(x) and 'items-center' in ' '.join(x)},
        
        # ë³µí•© ì¡°ê±´
        {'class': lambda x: x and any(cls in ' '.join(x) for cls in ['merchant', 'trader', 'npc'])},
    ]
    
    for i, pattern in enumerate(patterns, 1):
        try:
            containers = soup.find_all('div', pattern)
            print(f"íŒ¨í„´ {i}: {len(containers)}ê°œ ì»¨í…Œì´ë„ˆ ë°œê²¬")
            
            if containers:
                # ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ë¶„ì„
                first_container = containers[0]
                print(f"  ğŸ“¦ ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ í´ë˜ìŠ¤: {first_container.get('class', [])}")
                print(f"  ğŸ“ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {first_container.get_text()[:100]}...")
                
                # í•˜ìœ„ ìš”ì†Œë“¤ í™•ì¸
                child_divs = first_container.find_all('div')
                print(f"  ğŸ”— í•˜ìœ„ div ê°œìˆ˜: {len(child_divs)}")
                
                # ì´ë¯¸ì§€ ìš”ì†Œ í™•ì¸
                images = first_container.find_all('img')
                print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜: {len(images)}")
                
                if images:
                    for img in images[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                        alt_text = img.get('alt', '')
                        src = img.get('src', '')
                        print(f"    - ì´ë¯¸ì§€: alt='{alt_text}', src='{src[:50]}...'")
                
                print()
                
        except Exception as e:
            print(f"íŒ¨í„´ {i} ì˜¤ë¥˜: {e}")

def extract_merchant_info_attempt(html_content):
    """ìƒì¸ ì •ë³´ ì¶”ì¶œ ì‹œë„"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    print("\nğŸ£ ìƒì¸ ì •ë³´ ì¶”ì¶œ ì‹œë„:")
    print("=" * 50)
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ì§€ì—­ëª…ì´ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
    regions = ['ì•„ë¥´í…Œë¯¸ìŠ¤', 'ìœ ë””ì•„', 'ë£¨í…Œë€', 'í† í† ì´í¬', 'ì• ë‹ˆì¸ ', 'í˜ì´íŠ¼', 'í‘¸ë¥¸ë°”ë‹¤']
    
    merchants_found = []
    
    for region in regions:
        # í•´ë‹¹ ì§€ì—­ëª…ì„ í¬í•¨í•œ ìš”ì†Œë“¤ ì°¾ê¸°
        elements = soup.find_all(text=lambda text: text and region in text)
        
        if elements:
            print(f"\nğŸ“ {region} ê´€ë ¨ ìš”ì†Œ {len(elements)}ê°œ ë°œê²¬:")
            
            for element in elements[:3]:  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
                parent = element.parent
                if parent:
                    print(f"  - ë¶€ëª¨ íƒœê·¸: {parent.name}")
                    print(f"  - ë¶€ëª¨ í´ë˜ìŠ¤: {parent.get('class', [])}")
                    print(f"  - í…ìŠ¤íŠ¸: {str(element).strip()[:100]}...")
                    
                    # ìƒìœ„ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                    container = parent
                    for _ in range(5):  # ìµœëŒ€ 5ë‹¨ê³„ ìƒìœ„ê¹Œì§€
                        if container.parent and container.parent.name == 'div':
                            container = container.parent
                            classes = container.get('class', [])
                            if any(cls in ' '.join(classes) for cls in ['px-', 'py-', 'border-', 'flex']):
                                print(f"    â†’ ìƒìœ„ ì»¨í…Œì´ë„ˆ: {classes}")
                                break
                    
                    print()

def save_html_for_analysis(html_content):
    """ë¶„ì„ìš© HTML íŒŒì¼ ì €ì¥"""
    filename = f"kloa_merchant_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
    filepath = f"/mnt/c/Users/user/Documents/ì¹´ì¹´ì˜¤í†¡ ë°›ì€ íŒŒì¼/ë””ìŠ¤ì½”ë“œë´‡/{filename}"
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"ğŸ’¾ HTML íŒŒì¼ ì €ì¥ë¨: {filename}")
        return filepath
    except Exception as e:
        print(f"âŒ HTML íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ KLOA ì›¹ì‚¬ì´íŠ¸ HTML íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. HTML ê°€ì ¸ì˜¤ê¸°
    html_content = fetch_kloa_merchant_page()
    
    if not html_content:
        print("âŒ HTMLì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    # 2. HTML íŒŒì¼ ì €ì¥
    saved_file = save_html_for_analysis(html_content)
    
    # 3. HTML êµ¬ì¡° ë¶„ì„
    analyze_html_structure(html_content)
    
    # 4. ìƒì¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
    find_merchant_containers(html_content)
    
    # 5. ìƒì¸ ì •ë³´ ì¶”ì¶œ ì‹œë„
    extract_merchant_info_attempt(html_content)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    if saved_file:
        print(f"ğŸ“ ì €ì¥ëœ HTML íŒŒì¼ì„ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”: {saved_file}")

if __name__ == "__main__":
    main()
